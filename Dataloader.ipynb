{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from transformers import FlaxCLIPModel\n",
    "# from src.step_utils import CLIPProcessor\n",
    "import jax\n",
    "from flax.training import common_utils, train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV creation (with image names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "path = './putting-nerf-on-a-diet/data/phototourism/notre/images'\n",
    "\n",
    "data = {\n",
    "    'image_name': []\n",
    "}\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for filename in files:\n",
    "        data['image_name'].append(str(filename))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('phototourism_notre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>omaromar_11727516.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>jtriefen_370744178.rd.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>m500_2346149454.rd.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>algreen_2068599265.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13284673@N00_416967273.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  image_name\n",
       "0           0       omaromar_11727516.jpg\n",
       "1           1   jtriefen_370744178.rd.jpg\n",
       "2           2      m500_2346149454.rd.jpg\n",
       "3           3      algreen_2068599265.jpg\n",
       "4           4  13284673@N00_416967273.jpg"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('putting-nerf-on-a-diet/data/phototourism/notre/phototourism_notre.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF_Dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.img_names = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.img_names.iloc[idx, 1])      # Image name is in the 1th column of the csv file, so grabbing image name from idx row\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        sample = {'image': image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample['image'])\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Need to make all the images of same shape, so dataloader can efficiently load\n",
    "img_size = [300, 300]\n",
    "data_transform = transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "dataset = NeRF_Dataset(csv_file='putting-nerf-on-a-diet/data/phototourism/notre/phototourism_notre.csv',\n",
    "                                    root_dir='putting-nerf-on-a-diet/data/phototourism/notre/images',\n",
    "                                    transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16,\n",
    "                        shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype = np.float16)\n",
    "\n",
    "state = {\n",
    "    'imgs': [],\n",
    "    'embeded_imgs': []\n",
    "}\n",
    "\n",
    "def embed_images(state, images):\n",
    "\n",
    "    # CLIP_model = FlaxCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", dtype = np.float16)\n",
    "\n",
    "    def CLIPProcessor(image):\n",
    "        '''\n",
    "            jax-based preprocessing for CLIP\n",
    "\n",
    "            image  [B, 3, H, W]: batch image\n",
    "            return [B, 3, 224, 224]: pre-processed image for CLIP\n",
    "        '''\n",
    "        B,D,H,W = image.shape\n",
    "        image = jax.image.resize(image, (B,D,224,224), 'bicubic') # assume that images have rectangle shape. \n",
    "        mean = np.array([0.48145466, 0.4578275, 0.40821073]).reshape(1,3,1,1)\n",
    "        std = np.array([0.26862954, 0.26130258, 0.27577711]).reshape(1,3,1,1)\n",
    "        image = (image - mean.astype(image.dtype)) / std.astype(image.dtype) \n",
    "        return image\n",
    "\n",
    "\n",
    "    for img in images:\n",
    "        img = img.transpose(1, 2, 0)  # changing shape from CxHxW to HxWxC\n",
    "        state['imgs'].append(img)\n",
    "        H, W = img.shape[:2]\n",
    "        i, j = np.meshgrid(np.arange(0, W, 4), np.arange(0, H, 4), indexing='xy')\n",
    "        images = img[j, i]\n",
    "        images /= 255.\n",
    "        target_emb = CLIP_model.get_image_features(pixel_values=CLIPProcessor(np.expand_dims(images,0).transpose(0,3,1,2)))\n",
    "        target_emb /= np.linalg.norm(target_emb, axis=-1, keepdims=True)\n",
    "        state['embeded_imgs'].append(target_emb)\n",
    "    \n",
    "    return state\n",
    "\n",
    "# p_embed_images = jax.pmap(embed_images)\n",
    "# j_embed_images = jax.jit(embed_images)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for images in dataloader:\n",
    "    embed_images(state, np.array(images))\n",
    "    # state = p_embed_images(state, images, CLIP_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}